{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "553a61c6-f5f9-407e-a29a-2604b3913e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1Ridge regression is a model tuning method that is used to analyse any data that suffers from multicollinearity. This method performs L2 regularization. When the issue of multicollinearity occurs, least-squares are unbiased, and variances are large, this results in predicted values being far away from the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "224fb008-e8c5-413a-b590-1ee20afd1ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 Ridge Regression is a statistical method that assumes linearity, constant variance, and independence.1 However, it does not provide confidence limits, so the distribution of errors to be normal does not need to be assumed.125 Ridge Regression assumes that the number of predictors is less than the number of observations. Violations of these assumptions can lead to biased or inefficient estimates and impact the accuracy of the model.4 The bias and variance of the ridge estimator are derived under the commonly made assumption that, conditional on, the errors of the regression have zero mean and constant variance and are uncorrelated.0 If the assumption of normality is replaced by assumptions of homoscedasticity and uncorrelatedness of errors, and if one still assumes zero mean, then the Gauss-Markov theorem entails that the solution is the minimal unbiased linear estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c388803c-9c76-4473-b462-973e13ae3777",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 In ridge regression, we add a penalty by way of a tuning parameter called lambda which is chosen using cross validation. The idea is to make the fit small by making the residual sum or squares small plus adding a shrinkage penalty. The shrinkage penalty is lambda times the sum of squares of the coefficients so coefficients that get too large are penalized. As lambda gets larger, the bias is unchanged but the variance drops. The drawback of ridge is that it doesn’t select variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01967e79-e555-4215-9aac-89eb7630c1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4We can use ridge regression for feature selection while fitting the model. In this article, we are going to use logistic regression for model fitting and push the parameter penalty as L2 which basically means the penalty we use in ridge regression. · ridge_logit =LogisticRegression(C=1, penalty='l2') ridge_logit.fit(X_train, y_train) ... Let’s check the coefficient of features that will tell us how the features are important for the model. ... In the above, we can see that the feature from the data is one of the most important features and other features are not that much important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cddb269-d65f-4a99-ad7a-7acf4891ac77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 Consider the simple case of 2 predictor variables (x1\n",
    "#, x2\n",
    "#). If there is no or little colinearity and good spread in both predictors, then we are fitting a plane to the data (y\n",
    "# is the 3rd dimension) and there is often a very clear \"best\" plane. But with colinearity the relationship is really a line through 3 dimensional space with data scattered around it. But the regression routine tries to fit a plane to a line, so there are an infinite number of planes that intersect perfectly with that line, which plane is chosen depends on the influential points in the data, change one of those points just a little and the \"best\" fitting plane changes quite a bit. What ridge regression does is to pull the chosen plane towards simpler/saner models (bias values towards 0). Think of a rubber band from the origin (0,0,0) to the plane that pulls the plane towards 0 while the data will pull it away for a nice compromise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8705c91e-8825-4505-80ef-87dcbe80a545",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 you are correct to assume that a categorical variable is encoded as indicator function/vector in your design matrix X\n",
    "#; this is standard. To that respect, usually one of the levels is omitted and subsequently treated as baseline (if not you would have surely a rank-deficient design matrix when incorporating an intercept).\n",
    "\n",
    "#If you have a categorical variable with multiple categories you will once more treat is as an indicator function in your design matrix X\n",
    "#. Just now you will not have a vector but a smaller submatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcd54df2-1484-4b33-bac5-b59cc0c786e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7 Ridge Regression is a method that penalizes the coefficients of a linear model by shrinking them towards zero.2 This method eliminates collinearity, leading to more precise and interpretable parameter estimates. However, there is a trade-off between variance and bias, and the bias introduced by ridge regression is almost always toward the null.0 The coefficients of ridge regression can be visualized by plotting them against different values of lambda or alpha to see how regularization affects the coefficients and how they change from ridge to lasso regression.1 Lasso regression can lower the coefficients to zero resulting in feature selection, while ridge regression cannot result in feature selection as it only gets the coefficients close to zero.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe942a8-0eed-47ac-aa16-0b07397b7938",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
